{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dependencies, defining functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pickle\n",
    "import os.path\n",
    "from newspaper import Article\n",
    "import re\n",
    "\n",
    "def get_stopfake_links(url, start_page=1,end_page=2):\n",
    "    '''Iterate over given page range on https://www.stopfake.org, \n",
    "    getting all div-containers with class <cont-img>. \n",
    "    Downloads all links contained in <a> tags.\n",
    "    Returns list of links'''\n",
    "    links = []\n",
    "    for p_number in range(start_page,end_page):\n",
    "        url_temp = url + str(p_number)\n",
    "        page = requests.get(url_temp)\n",
    "        soup = BeautifulSoup(page.text, 'lxml')    \n",
    "        div_list = soup.find_all(\"div\", \"cont-img\")\n",
    "        for el in div_list:\n",
    "            link = el.find('a', href=True).get('href')\n",
    "            if 'stopfakenews' in link:\n",
    "                continue\n",
    "            else:\n",
    "                links.append(link)\n",
    "        print('page processed: '+ str(p_number))\n",
    "    return links\n",
    "\n",
    "def get_fake_links(urls):\n",
    "    '''Iterate over list passed. Reads only first <a> tag contained\n",
    "    in div with class <post-content>. Last loop gets rid of junk links like Facebook, Google. \n",
    "    Returns list of links.'''\n",
    "    fake_links = []\n",
    "    total_length = len(urls)\n",
    "    for x in range(len(urls)):  \n",
    "        counter = 0\n",
    "        page = requests.get(urls[x])\n",
    "        soup = BeautifulSoup(page.text, 'lxml')\n",
    "        div_list = soup.find_all(\"div\", \"post-content\")\n",
    "        for el in div_list:    \n",
    "            counter +=1\n",
    "            f_link = el.find('a', href=True)\n",
    "            if counter == 1 and f_link != None:\n",
    "                fake_links.append(f_link.get('href'))\n",
    "            else: \n",
    "                continue\n",
    "        print('Links processed: '+ str(round(((x/total_length)*100),2))+'%')\n",
    "    fake_links_cleaned = [link for link in fake_links if 'stopfake' not in link if 'facebook' not in link if 'youtube' not in link if 'google' not in link]\n",
    "    return fake_links_cleaned\n",
    "\n",
    "\n",
    "def download_articles(url):\n",
    "    '''Iterate over links with news/fakenews. If there is no problem with link,\n",
    "    download article. Returns dictionary with \"keys\" - URLs and \"values\" -  text strings'''\n",
    "    text_parsed = {}\n",
    "    total_length = len(url)\n",
    "    for x in range(len(url)):\n",
    "        try:\n",
    "            req = requests.get(url[x])\n",
    "        except (requests.exceptions.ConnectionError,requests.exceptions.SSLError) as error:\n",
    "            print(error)\n",
    "        try:\n",
    "            if req.status_code == 200:\n",
    "                article = Article(url[x], language='ru')\n",
    "                article.download(),article.parse()\n",
    "                text_parsed[url[x]] = article.text\n",
    "            else:\n",
    "                continue\n",
    "        except (BaseException) as error:\n",
    "            print(error)\n",
    "        print('Links processed: '+ str(round(((x/total_length)*100),2))+'%')\n",
    "    return text_parsed\n",
    "\n",
    "def text_serializer(filename,text):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(text, file)\n",
    "        \n",
    "def text_deserializer(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data scrapping from StopFake.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving links on StopFake rebuttal articles from https://stopfake.org\n",
    "stopfake_links = get_stopfake_links('http://www.stopfake.org/category/novosti/page/', 1,1600)\n",
    "print(len(stopfake_links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Saving links on original fakes. 90% of links are first URL of each StopFake article.  \n",
    "fakenews_links = get_fake_links(stopfake_links)\n",
    "print(len(fakenews_links))\n",
    "#text_serializer(os.path.join('raw_data', 'fakenews_links_#898_aug_11_2017'), fakenews_links)\n",
    "#fakenews_links = text_deserializer(os.path.join('raw_data', 'fakenews_links_#898_aug_11_2017'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Downloading original fakes from fakenews_links\n",
    "fake_news_text = download_articles(fakenews_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_serializer(os.path.join('raw_data', 'faketext_#807_aug_11_2017'), fake_news_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
