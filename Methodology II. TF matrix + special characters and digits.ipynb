{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dependencies and data files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import svm\n",
    "import math\n",
    "import re\n",
    "\n",
    "\n",
    "fake, non_fake  = pd.read_excel('cleaned_files/Fakeset_cleaned_165.xlsx'), pd.read_excel('cleaned_files/Non-fakeset_cleaned_318.xlsx')\n",
    "\n",
    "dataset = pd.concat([fake, non_fake])\n",
    "\n",
    "pravda_unian_set = pd.read_excel('cleaned_files/Non-fakeset.xlsx')\n",
    "\n",
    "testing_set = pravda_unian_set[(pravda_unian_set['url'].isin(dataset['url']) == False)]\n",
    "\n",
    "dataset.reset_index(np.linspace(1,len(dataset)), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    2.253235\n",
       "1    1.753799\n",
       "Name: digit_freq, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Creating new features\n",
    "\n",
    "#dataset['numbers_freq'] = dataset['text'].str.findall('(\\d{1,2})+').apply(lambda x: len(x)/len(dataset['text']))\n",
    "\n",
    "def build_log_scale(row):\n",
    "    scale = pow(len(row['text']), 1/10)\n",
    "    spmarks_ratio = len(re.findall('[,.?!:;\"]', row['text']))\n",
    "    score = math.log(spmarks_ratio, scale)\n",
    "    return score\n",
    "\n",
    "def build_digit_scale(row):\n",
    "    scale = pow(len(row['text']), 1/10)\n",
    "    digit_ratio = len(re.findall('(\\d{1,2})+', row['text']))\n",
    "    if digit_ratio == 0:\n",
    "        score = 1\n",
    "    else:\n",
    "        score = math.log(digit_ratio, scale)\n",
    "    return score\n",
    "\n",
    "dataset['special_marks'] = dataset.apply(build_log_scale, axis=1)\n",
    "dataset['digit_freq'] = dataset.apply(build_digit_scale, axis=1)\n",
    "\n",
    "dataset.groupby('target')['digit_freq'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-test on means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=4.5733469250621788, pvalue=6.113492406924028e-06)\n",
      "Ttest_indResult(statistic=-2.9533141955616653, pvalue=0.0032977225893373011)\n",
      "MannwhitneyuResult(statistic=22408.5, pvalue=0.0042691134996715669)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "g0 = dataset.groupby('target').get_group(0)\n",
    "g1 = dataset.groupby('target').get_group(1)\n",
    "\n",
    "print(ttest_ind(g0['digit_freq'], g1['digit_freq']))\n",
    "print(ttest_ind(g0['special_marks'], g1['special_marks']))\n",
    "print(mannwhitneyu(g0['special_marks'], g1['special_marks']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiating word vectorizer + stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(language='russian')\n",
    "\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc) if w not in rus_stopwords)\n",
    "\n",
    "rus_stopwords = [\"c\",\"а\",\"алло\",\"без\",\"белый\",\"близко\",\"более\",\"больше\",\"большой\",\"будем\",\"будет\",\"будете\",\"будешь\",\"будто\",\"буду\",\"будут\",\"будь\",\"бы\",\"бывает\",\"бывь\",\"был\",\"была\",\"были\",\"было\",\"быть\",\"в\",\"важная\",\"важное\",\"важные\",\"важный\",\"вам\",\"вами\",\"вас\",\"ваш\",\"ваша\",\"ваше\",\"ваши\",\"вверх\",\"вдали\",\"вдруг\",\"ведь\",\"везде\",\"вернуться\",\"весь\",\"вечер\",\"взгляд\",\"взять\",\"вид\",\"видел\",\"видеть\",\"вместе\",\"вне\",\"вниз\",\"внизу\",\"во\",\"вода\",\"война\",\"вокруг\",\"вон\",\"вообще\",\"вопрос\",\"восемнадцатый\",\"восемнадцать\",\"восемь\",\"восьмой\",\"вот\",\"впрочем\",\"времени\",\"время\",\"все\",\"все еще\",\"всегда\",\"всего\",\"всем\",\"всеми\",\"всему\",\"всех\",\"всею\",\"всю\",\"всюду\",\"вся\",\"всё\",\"второй\",\"вы\",\"выйти\",\"г\",\"где\",\"главный\",\"глаз\",\"говорил\",\"говорит\",\"говорить\",\"год\",\"года\",\"году\",\"голова\",\"голос\",\"город\",\"да\",\"давать\",\"давно\",\"даже\",\"далекий\",\"далеко\",\"дальше\",\"даром\",\"дать\",\"два\",\"двадцатый\",\"двадцать\",\"две\",\"двенадцатый\",\"двенадцать\",\"дверь\",\"двух\",\"девятнадцатый\",\"девятнадцать\",\"девятый\",\"девять\",\"действительно\",\"дел\",\"делал\",\"делать\",\"делаю\",\"дело\",\"день\",\"деньги\",\"десятый\",\"десять\",\"для\",\"до\",\"довольно\",\"долго\",\"должен\",\"должно\",\"должный\",\"дом\",\"дорога\",\"друг\",\"другая\",\"другие\",\"других\",\"друго\",\"другое\",\"другой\",\"думать\",\"душа\",\"е\",\"его\",\"ее\",\"ей\",\"ему\",\"если\",\"есть\",\"еще\",\"ещё\",\"ею\",\"её\",\"ж\",\"ждать\",\"же\",\"жена\",\"женщина\",\"жизнь\",\"жить\",\"за\",\"занят\",\"занята\",\"занято\",\"заняты\",\"затем\",\"зато\",\"зачем\",\"здесь\",\"земля\",\"знать\",\"значит\",\"значить\",\"и\",\"иди\",\"идти\",\"из\",\"или\",\"им\",\"имеет\",\"имел\",\"именно\",\"иметь\",\"ими\",\"имя\",\"иногда\",\"их\",\"к\",\"каждая\",\"каждое\",\"каждые\",\"каждый\",\"кажется\",\"казаться\",\"как\",\"какая\",\"какой\",\"кем\",\"книга\",\"когда\",\"кого\",\"ком\",\"комната\",\"кому\",\"конец\",\"конечно\",\"которая\",\"которого\",\"которой\",\"которые\",\"который\",\"которых\",\"кроме\",\"кругом\",\"кто\",\"куда\",\"лежать\",\"лет\",\"ли\",\"лицо\",\"лишь\",\"лучше\",\"любить\",\"люди\",\"м\",\"маленький\",\"мало\",\"мать\",\"машина\",\"между\",\"меля\",\"менее\",\"меньше\",\"меня\",\"место\",\"миллионов\",\"мимо\",\"минута\",\"мир\",\"мира\",\"мне\",\"много\",\"многочисленная\",\"многочисленное\",\"многочисленные\",\"многочисленный\",\"мной\",\"мною\",\"мог\",\"могу\",\"могут\",\"мож\",\"может\",\"может быть\",\"можно\",\"можхо\",\"мои\",\"мой\",\"мор\",\"москва\",\"мочь\",\"моя\",\"моё\",\"мы\",\"на\",\"наверху\",\"над\",\"надо\",\"назад\",\"наиболее\",\"найти\",\"наконец\",\"нам\",\"нами\",\"народ\",\"нас\",\"начала\",\"начать\",\"наш\",\"наша\",\"наше\",\"наши\",\"не\",\"него\",\"недавно\",\"недалеко\",\"нее\",\"ней\",\"некоторый\",\"нельзя\",\"нем\",\"немного\",\"нему\",\"непрерывно\",\"нередко\",\"несколько\",\"нет\",\"нею\",\"неё\",\"ни\",\"нибудь\",\"ниже\",\"низко\",\"никакой\",\"никогда\",\"никто\",\"никуда\",\"ним\",\"ними\",\"них\",\"ничего\",\"ничто\",\"но\",\"новый\",\"нога\",\"ночь\",\"ну\",\"нужно\",\"нужный\",\"нх\",\"о\",\"об\",\"оба\",\"обычно\",\"один\",\"одиннадцатый\",\"одиннадцать\",\"однажды\",\"однако\",\"одного\",\"одной\",\"оказаться\",\"окно\",\"около\",\"он\",\"она\",\"они\",\"оно\",\"опять\",\"особенно\",\"остаться\",\"от\",\"ответить\",\"отец\",\"откуда\",\"отовсюду\",\"отсюда\",\"очень\",\"первый\",\"перед\",\"писать\",\"плечо\",\"по\",\"под\",\"подойди\",\"подумать\",\"пожалуйста\",\"позже\",\"пойти\",\"пока\",\"пол\",\"получить\",\"помнить\",\"понимать\",\"понять\",\"пор\",\"пора\",\"после\",\"последний\",\"посмотреть\",\"посреди\",\"потом\",\"потому\",\"почему\",\"почти\",\"правда\",\"прекрасно\",\"при\",\"про\",\"просто\",\"против\",\"процентов\",\"путь\",\"пятнадцатый\",\"пятнадцать\",\"пятый\",\"пять\",\"работа\",\"работать\",\"раз\",\"разве\",\"рано\",\"раньше\",\"ребенок\",\"решить\",\"россия\",\"рука\",\"русский\",\"ряд\",\"рядом\",\"с\",\"с кем\",\"сам\",\"сама\",\"сами\",\"самим\",\"самими\",\"самих\",\"само\",\"самого\",\"самой\",\"самом\",\"самому\",\"саму\",\"самый\",\"свет\",\"свое\",\"своего\",\"своей\",\"свои\",\"своих\",\"свой\",\"свою\",\"сделать\",\"сеаой\",\"себе\",\"себя\",\"сегодня\",\"седьмой\",\"сейчас\",\"семнадцатый\",\"семнадцать\",\"семь\",\"сидеть\",\"сила\",\"сих\",\"сказал\",\"сказала\",\"сказать\",\"сколько\",\"слишком\",\"слово\",\"случай\",\"смотреть\",\"сначала\",\"снова\",\"со\",\"собой\",\"собою\",\"советский\",\"совсем\",\"спасибо\",\"спросить\",\"сразу\",\"стал\",\"старый\",\"стать\",\"стол\",\"сторона\",\"стоять\",\"страна\",\"суть\",\"считать\",\"т\",\"та\",\"так\",\"такая\",\"также\",\"таки\",\"такие\",\"такое\",\"такой\",\"там\",\"твои\",\"твой\",\"твоя\",\"твоё\",\"те\",\"тебе\",\"тебя\",\"тем\",\"теми\",\"теперь\",\"тех\",\"то\",\"тобой\",\"тобою\",\"товарищ\",\"тогда\",\"того\",\"тоже\",\"только\",\"том\",\"тому\",\"тот\",\"тою\",\"третий\",\"три\",\"тринадцатый\",\"тринадцать\",\"ту\",\"туда\",\"тут\",\"ты\",\"тысяч\",\"у\",\"увидеть\",\"уж\",\"уже\",\"улица\",\"уметь\",\"утро\",\"хороший\",\"хорошо\",\"хотел бы\",\"хотеть\",\"хоть\",\"хотя\",\"хочешь\",\"час\",\"часто\",\"часть\",\"чаще\",\"чего\",\"человек\",\"чем\",\"чему\",\"через\",\"четвертый\",\"четыре\",\"четырнадцатый\",\"четырнадцать\",\"что\",\"чтоб\",\"чтобы\",\"чуть\",\"шестнадцатый\",\"шестнадцать\",\"шестой\",\"шесть\",\"эта\",\"эти\",\"этим\",\"этими\",\"этих\",\"это\",\"этого\",\"этой\",\"этом\",\"этому\",\"этот\",\"эту\",\"я\",\"являюсь\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding features to TF matrix; model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset[['ru_text', 'digit_freq', 'special_marks']], \n",
    "                                                    dataset['target'], \n",
    "                                                    random_state=0)\n",
    "\n",
    "vect = CountVectorizer(min_df=5, ngram_range=(1,3), analyzer=stemmed_words).fit(X_train['ru_text'])\n",
    "\n",
    "X_train_extended = add_feature(vect.transform(X_train['ru_text']), X_train['digit_freq'])\n",
    "\n",
    "X_train_extended = add_feature(X_train_extended, X_train['special_marks'])\n",
    "\n",
    "\n",
    "model = svm.SVC(kernel='linear', C=1).fit(X_train_extended, y_train)\n",
    "\n",
    "X_test_extended = add_feature(vect.transform(X_test['ru_text']), X_test['digit_freq'])\n",
    "\n",
    "X_test_extended2 = add_feature(X_test_extended, X_test['special_marks'])\n",
    "\n",
    "predictions = model.predict(X_test_extended2)\n",
    "\n",
    "auc = roc_auc_score(y_test, predictions)\n",
    "\n",
    "accuracy = model.score(X_test_extended2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.97520661157024791, 'auc': 0.96633986928104576, 'test_set_accuracy': 0.7509025270758123}\n"
     ]
    }
   ],
   "source": [
    "testing_set['numbers_freq'] = testing_set['text'].str.findall('(\\d{1,2})+').apply(lambda x: len(x)/len(testing_set['ru_text']))\n",
    "\n",
    "testing_set['special_marks'] = testing_set.apply(build_log_scale, axis=1)\n",
    "\n",
    "testing = add_feature(vect.transform(testing_set['ru_text']), testing_set['numbers_freq'])\n",
    "\n",
    "testing2 = add_feature(testing, testing_set['special_marks'])\n",
    "\n",
    "res = model.predict(testing2)\n",
    "\n",
    "\n",
    "'''By adding extra features, the test set accuracy decreased by 0.03.'''\n",
    "print({'accuracy': accuracy, 'auc': auc, 'test_set_accuracy': (1-np.count_nonzero(res)/len(res))})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
