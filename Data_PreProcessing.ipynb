{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dependencies; defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def text_deserializer(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "def text_serializer(filename,text):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(text, file)\n",
    "        \n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "stemmer = RussianStemmer()\n",
    "stops = set(stopwords.words(\"russian\"))\n",
    "\n",
    "def words_and_chars(data):\n",
    "    '''Depreceated'''\n",
    "    special_marks = [re.findall(\"[,?.-]\", data[y]) for y in range(len(data))]\n",
    "    articles = [re.findall(\"[а-яА-Я]+\", data[y]) for y in range(len(data))]\n",
    "    stemmed_words = []\n",
    "    for article in articles:\n",
    "        words = [stemmer.stem(word) for word in article if word not in stops]\n",
    "        stemmed_words.append(words)\n",
    "    if len(special_marks) != len(stemmed_words):\n",
    "        print('Length of two lists is not equal. Check the data!')\n",
    "    return stemmed_words, special_marks\n",
    "      \n",
    "\n",
    "def filter_by_keywords(keywords, data, min_words=50, label=None):\n",
    "    '''Iterate over given article, looking for match in defined keywords, returns articles with matched words'''\n",
    "    dict_ret = {}\n",
    "    for key,value in data.items():\n",
    "        if (len(value) < min_words) or (value == '') or (value == ' '):\n",
    "            continue\n",
    "        else:\n",
    "            for keyword in keywords:\n",
    "                if keyword in value.lower():\n",
    "                    dict_ret[key] = value\n",
    "                else:\n",
    "                    continue\n",
    "    return {'url': [key for key in dict_ret.keys()], 'text': [text for text in dict_ret.values()], 'target': label}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Loading data\n",
    "\n",
    "fake_text = text_deserializer(os.path.join('raw_data', 'faketext_#807_aug_11_2017'))\n",
    "ukr_text_war = text_deserializer(os.path.join('raw_data', 'unian_ukrpravda_war_#1554_aug_11_2017'))\n",
    "ukr_text_minsk = text_deserializer(os.path.join('raw_data', 'unian_ukrpravda_minsk_#267_aug_11_2017'))\n",
    "test_set = text_deserializer(os.path.join('raw_data', 'test_set'))\n",
    "ukr_text = ukr_text_war.copy()\n",
    "ukr_text.update(ukr_text_minsk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering articles by keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['обсе', 'минский договор', 'нормандский', 'лнр', 'днр', 'плотницкий', 'захарченко', 'ополченцы', 'боевики','незалежная', 'народная республика', 'киев', 'силовики']\n",
    "\n",
    "fake = filter_by_keywords(keys, fake_text, label=1)\n",
    "\n",
    "non_fake = filter_by_keywords(keys, ukr_text, label=0)\n",
    "\n",
    "print(len(fake['text']))\n",
    "print(len(non_fake['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structuring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset_fake, dataset_non_fake  = pd.DataFrame(fake), pd.DataFrame(non_fake)\n",
    "\n",
    "dataset = pd.concat([dataset_fake, dataset_non_fake])\n",
    "\n",
    "dataset.sort_index(axis=1, inplace=True, ascending=False)\n",
    "\n",
    "dataset['ru_text'] = dataset['text'].str.findall('[а-яА-Я]*').str.join(' ')\n",
    "\n",
    "dataset['label'] = dataset['target'].apply(lambda x: 'Fake' if x == 1 else 'Not-Fake')\n",
    "\n",
    "dataset['url_shorten'] = dataset['url'].str.extract('(^h\\w*://\\w{3,10}.\\w{2,10}.)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Saving to Excel\n",
    "fake_set = dataset[dataset['target'] == 1]\n",
    "\n",
    "non_fake_set = dataset[dataset['target'] == 0]\n",
    "\n",
    "fake_set.to_excel('Fakeset.xlsx')\n",
    "\n",
    "non_fake_set.to_excel('Non-fakeset.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Saving Zik file to Excel\n",
    "\n",
    "zik = pd.DataFrame(list(test_set.items()), columns=['url', 'text'])\n",
    "zik.to_excel('cleaned_files/zik_news.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
